{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7165afc",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafe64c6",
   "metadata": {},
   "source": [
    "## 1. Basic Gradients\n",
    "\n",
    "### Scalar Functions - Single Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca821ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x) = x^2\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "print(f\"x = {x.item()}\")\n",
    "print(f\"y = x^2 = {y.item()}\")\n",
    "\n",
    "# Compute gradient dy/dx = 2x\n",
    "y.backward()\n",
    "print(f\"\\nGradient dy/dx = 2x = {x.grad.item()}\")\n",
    "print(f\"Expected: 2 * {x.item()} = {2 * x.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d0f32",
   "metadata": {},
   "source": [
    "### Polynomial Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7052464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x) = x^3 + 2x^2 + 3x + 1\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**3 + 2*x**2 + 3*x + 1\n",
    "\n",
    "print(f\"f(x) = x^3 + 2x^2 + 3x + 1\")\n",
    "print(f\"f({x.item()}) = {y.item()}\")\n",
    "\n",
    "y.backward()\n",
    "print(f\"\\nGradient f'(x) = 3x^2 + 4x + 3\")\n",
    "print(f\"f'({x.item()}) = {x.grad.item()}\")\n",
    "print(f\"Expected: 3*{x.item()}^2 + 4*{x.item()} + 3 = {3*x.item()**2 + 4*x.item() + 3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf1b195",
   "metadata": {},
   "source": [
    "## 2. Vector Functions\n",
    "\n",
    "### Gradient of Vector Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4222d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x) = x^T x (dot product)\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = torch.dot(x, x)  # x^T x\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = x^T x = {y.item()}\")\n",
    "\n",
    "y.backward()\n",
    "print(f\"\\nGradient ∇f = 2x\")\n",
    "print(f\"∇f = {x.grad}\")\n",
    "print(f\"Expected 2x = {2 * x.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48b843a",
   "metadata": {},
   "source": [
    "### Quadratic Form: x^T A x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b3a1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x) = x^T A x\n",
    "A = torch.tensor([[2.0, 1.0], [1.0, 3.0]])\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "\n",
    "y = x @ A @ x\n",
    "print(f\"A = \\n{A}\")\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = x^T A x = {y.item()}\")\n",
    "\n",
    "y.backward()\n",
    "print(f\"\\nGradient ∇f = (A + A^T)x\")\n",
    "print(f\"∇f = {x.grad}\")\n",
    "print(f\"Expected (A + A^T)x = {((A + A.T) @ x.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b90e87",
   "metadata": {},
   "source": [
    "## 3. Matrix Functions\n",
    "\n",
    "### Trace of Matrix Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce4fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(X) = tr(AX) where A is constant\n",
    "A = torch.randn(3, 3)\n",
    "X = torch.randn(3, 3, requires_grad=True)\n",
    "\n",
    "y = torch.trace(A @ X)\n",
    "print(f\"f(X) = tr(AX) = {y.item():.4f}\")\n",
    "\n",
    "y.backward()\n",
    "print(f\"\\nGradient ∇f = A^T\")\n",
    "print(f\"∇f = \\n{X.grad}\")\n",
    "print(f\"\\nExpected A^T = \\n{A.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f092f7",
   "metadata": {},
   "source": [
    "### Frobenius Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bfa0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(X) = ||X||_F^2 = tr(X^T X)\n",
    "X = torch.randn(3, 3, requires_grad=True)\n",
    "y = torch.norm(X, p='fro') ** 2\n",
    "\n",
    "print(f\"f(X) = ||X||_F^2 = {y.item():.4f}\")\n",
    "\n",
    "y.backward()\n",
    "print(f\"\\nGradient ∇f = 2X\")\n",
    "print(f\"∇f = \\n{X.grad}\")\n",
    "print(f\"\\nExpected 2X = \\n{2 * X.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3831220f",
   "metadata": {},
   "source": [
    "## 4. Chain Rule\n",
    "\n",
    "### Composite Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x) = sin(x^2)\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.sin(x ** 2)\n",
    "\n",
    "print(f\"f(x) = sin(x^2)\")\n",
    "print(f\"f({x.item()}) = {y.item():.4f}\")\n",
    "\n",
    "y.backward()\n",
    "print(f\"\\nGradient f'(x) = 2x * cos(x^2)\")\n",
    "print(f\"f'({x.item()}) = {x.grad.item():.4f}\")\n",
    "print(f\"Expected: 2*{x.item()}*cos({x.item()**2}) = {(2*x.item()*np.cos(x.item()**2)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bd4d53",
   "metadata": {},
   "source": [
    "### Nested Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491ba417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x) = exp(log(x^2 + 1))\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.exp(torch.log(x**2 + 1))\n",
    "\n",
    "print(f\"f(x) = exp(log(x^2 + 1))\")\n",
    "print(f\"f({x.item()}) = {y.item():.4f}\")\n",
    "\n",
    "y.backward()\n",
    "print(f\"\\nGradient f'(x) = 2x\")\n",
    "print(f\"f'({x.item()}) = {x.grad.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cecc0ed",
   "metadata": {},
   "source": [
    "## 5. Jacobian Matrix\n",
    "\n",
    "### Vector-Valued Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4af9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"f: R^2 -> R^2, f(x) = [x1^2 + x2, x1 * x2]\"\"\"\n",
    "    return torch.stack([x[0]**2 + x[1], x[0] * x[1]])\n",
    "\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = f(x)\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"f(x) = {y}\")\n",
    "\n",
    "# Compute Jacobian\n",
    "jacobian = torch.autograd.functional.jacobian(f, x)\n",
    "print(f\"\\nJacobian J = \")\n",
    "print(jacobian)\n",
    "print(f\"\\nExpected:\")\n",
    "print(f\"[2*x1,   1  ] = [{2*x[0].item():.1f}, {1.0}]\")\n",
    "print(f\"[ x2,   x1  ] = [{x[1].item():.1f}, {x[0].item():.1f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4524b45",
   "metadata": {},
   "source": [
    "### Matrix-Valued Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6747b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_func(x):\n",
    "    \"\"\"f: R^2 -> R^(2x2)\"\"\"\n",
    "    return torch.tensor([[x[0]**2, x[0]*x[1]], \n",
    "                        [x[1]**2, x[0]+x[1]]])\n",
    "\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "y = matrix_func(x)\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"f(x) = \\n{y}\")\n",
    "\n",
    "# Compute Jacobian\n",
    "jacobian = torch.autograd.functional.jacobian(matrix_func, x)\n",
    "print(f\"\\nJacobian shape: {jacobian.shape}\")\n",
    "print(f\"Jacobian:\\n{jacobian}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b97c72",
   "metadata": {},
   "source": [
    "## 6. Hessian Matrix\n",
    "\n",
    "### Second-Order Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab0437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x, y) = x^2 + xy + y^2\n",
    "def f(x):\n",
    "    return x[0]**2 + x[0]*x[1] + x[1]**2\n",
    "\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "y = f(x)\n",
    "\n",
    "print(f\"f(x) = x1^2 + x1*x2 + x2^2\")\n",
    "print(f\"f({x[0].item()}, {x[1].item()}) = {y.item()}\")\n",
    "\n",
    "# Compute Hessian\n",
    "hessian = torch.autograd.functional.hessian(f, x)\n",
    "print(f\"\\nHessian H = \")\n",
    "print(hessian)\n",
    "print(f\"\\nExpected:\")\n",
    "print(\"[2, 1]\")\n",
    "print(\"[1, 2]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1056699",
   "metadata": {},
   "source": [
    "### Quadratic Function Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d47d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x) = 1/2 x^T A x where A is symmetric\n",
    "A = torch.tensor([[2.0, 1.0], [1.0, 3.0]])\n",
    "\n",
    "def quadratic(x):\n",
    "    return 0.5 * x @ A @ x\n",
    "\n",
    "x = torch.tensor([1.0, 1.0], requires_grad=True)\n",
    "y = quadratic(x)\n",
    "\n",
    "print(f\"f(x) = 1/2 x^T A x\")\n",
    "print(f\"A = \\n{A}\")\n",
    "print(f\"f(x) = {y.item()}\")\n",
    "\n",
    "# Hessian should equal A\n",
    "hessian = torch.autograd.functional.hessian(quadratic, x)\n",
    "print(f\"\\nHessian H = \")\n",
    "print(hessian)\n",
    "print(f\"\\nExpected H = A:\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c130f8",
   "metadata": {},
   "source": [
    "## 7. Directional Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f5716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directional derivative: D_v f(x) = ∇f(x) · v\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "v = torch.tensor([1.0, 0.0])  # Direction vector\n",
    "\n",
    "# Function f(x,y) = x^2 + y^2\n",
    "y = x[0]**2 + x[1]**2\n",
    "\n",
    "print(f\"f(x,y) = x^2 + y^2\")\n",
    "print(f\"Point x = {x}\")\n",
    "print(f\"Direction v = {v}\")\n",
    "\n",
    "# Compute gradient\n",
    "y.backward()\n",
    "grad = x.grad\n",
    "print(f\"\\nGradient ∇f = {grad}\")\n",
    "\n",
    "# Directional derivative\n",
    "dir_deriv = torch.dot(grad, v)\n",
    "print(f\"Directional derivative D_v f = ∇f · v = {dir_deriv.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10e5199",
   "metadata": {},
   "source": [
    "## 8. Gradient Descent Example\n",
    "\n",
    "### Minimizing a Quadratic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de3657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize f(x) = (x-2)^2 + (y-3)^2\n",
    "x = torch.tensor([0.0, 0.0], requires_grad=True)\n",
    "learning_rate = 0.1\n",
    "iterations = 50\n",
    "\n",
    "history = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Compute function\n",
    "    y = (x[0] - 2)**2 + (x[1] - 3)**2\n",
    "    history.append((x.data.clone(), y.item()))\n",
    "    \n",
    "    # Compute gradients\n",
    "    if x.grad is not None:\n",
    "        x.grad.zero_()\n",
    "    y.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iter {i}: x = [{x[0].item():.4f}, {x[1].item():.4f}], f(x) = {y.item():.4f}\")\n",
    "\n",
    "print(f\"\\nFinal: x = [{x[0].item():.4f}, {x[1].item():.4f}]\")\n",
    "print(f\"Expected minimum: [2.0, 3.0]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6e8287",
   "metadata": {},
   "source": [
    "### Visualize Gradient Descent Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9988fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract path\n",
    "path = torch.stack([h[0] for h in history])\n",
    "\n",
    "# Create contour plot\n",
    "x_vals = torch.linspace(-1, 4, 100)\n",
    "y_vals = torch.linspace(-1, 5, 100)\n",
    "X, Y = torch.meshgrid(x_vals, y_vals, indexing='ij')\n",
    "Z = (X - 2)**2 + (Y - 3)**2\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contour(X.numpy(), Y.numpy(), Z.numpy(), levels=20, cmap='viridis')\n",
    "plt.colorbar(label='f(x,y)')\n",
    "plt.plot(path[:, 0].numpy(), path[:, 1].numpy(), 'r.-', linewidth=2, markersize=8, label='Gradient Descent Path')\n",
    "plt.plot(2, 3, 'g*', markersize=20, label='Minimum')\n",
    "plt.plot(0, 0, 'ro', markersize=10, label='Start')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Gradient Descent on f(x,y) = (x-2)² + (y-3)²', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6bb2a",
   "metadata": {},
   "source": [
    "## 9. Newton's Method\n",
    "\n",
    "### Second-Order Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b21a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton's method: x_{k+1} = x_k - H^{-1} ∇f\n",
    "def f_scalar(x):\n",
    "    return x[0]**2 + 2*x[1]**2 + x[0]*x[1]\n",
    "\n",
    "x = torch.tensor([5.0, 5.0], requires_grad=True)\n",
    "iterations = 10\n",
    "\n",
    "print(\"Newton's Method for optimization:\")\n",
    "print(f\"f(x,y) = x^2 + 2y^2 + xy\\n\")\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Compute gradient and Hessian\n",
    "    grad = torch.autograd.functional.jacobian(f_scalar, x)\n",
    "    hess = torch.autograd.functional.hessian(f_scalar, x)\n",
    "    \n",
    "    # Newton update\n",
    "    delta = torch.linalg.solve(hess, grad)\n",
    "    x_new = x - delta\n",
    "    \n",
    "    f_val = f_scalar(x)\n",
    "    print(f\"Iter {i}: x = [{x[0].item():.6f}, {x[1].item():.6f}], f(x) = {f_val:.8f}\")\n",
    "    \n",
    "    x = x_new.detach().requires_grad_(True)\n",
    "    \n",
    "    # Check convergence\n",
    "    if torch.norm(delta) < 1e-10:\n",
    "        print(f\"\\nConverged at iteration {i}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nFinal: x = [{x[0].item():.10f}, {x[1].item():.10f}]\")\n",
    "print(f\"Expected minimum: [0.0, 0.0]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6708b6",
   "metadata": {},
   "source": [
    "## 10. Backpropagation in Neural Networks\n",
    "\n",
    "### Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d97ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-layer neural network\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2, 3)\n",
    "        self.layer2 = nn.Linear(3, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Create model and data\n",
    "model = SimpleNet()\n",
    "x = torch.tensor([[1.0, 2.0]])\n",
    "y_true = torch.tensor([[5.0]])\n",
    "\n",
    "# Forward pass\n",
    "y_pred = model(x)\n",
    "loss = (y_pred - y_true)**2\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Predicted: {y_pred.item():.4f}\")\n",
    "print(f\"True: {y_true.item()}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\nGradients:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0612f0e7",
   "metadata": {},
   "source": [
    "## 11. Matrix Derivatives - Common Formulas\n",
    "\n",
    "### Linear Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ∇(a^T x) = a\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "x = torch.tensor([2.0, 1.0, 0.5], requires_grad=True)\n",
    "\n",
    "y = torch.dot(a, x)\n",
    "print(f\"f(x) = a^T x = {y.item()}\")\n",
    "\n",
    "y.backward()\n",
    "print(f\"\\n∇f = a\")\n",
    "print(f\"∇f = {x.grad}\")\n",
    "print(f\"Expected: {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c88c15f",
   "metadata": {},
   "source": [
    "### Matrix-Vector Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b86cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ∇(x^T A x) = (A + A^T)x\n",
    "A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "x = torch.tensor([1.0, 1.0], requires_grad=True)\n",
    "\n",
    "y = x @ A @ x\n",
    "print(f\"f(x) = x^T A x = {y.item()}\")\n",
    "\n",
    "y.backward()\n",
    "print(f\"\\n∇f = (A + A^T)x\")\n",
    "print(f\"∇f = {x.grad}\")\n",
    "print(f\"Expected: {(A + A.T) @ x.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd14b2",
   "metadata": {},
   "source": [
    "### Determinant Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d6363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ∇ log|det(X)| = X^{-T}\n",
    "X = torch.tensor([[2.0, 1.0], [1.0, 2.0]], requires_grad=True)\n",
    "\n",
    "y = torch.log(torch.linalg.det(X))\n",
    "print(f\"f(X) = log|det(X)| = {y.item():.4f}\")\n",
    "\n",
    "y.backward()\n",
    "print(f\"\\n∇f = X^{{-T}}\")\n",
    "print(f\"∇f = \\n{X.grad}\")\n",
    "print(f\"\\nExpected X^{{-T}} = \\n{torch.linalg.inv(X.data).T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9369329",
   "metadata": {},
   "source": [
    "## 12. Visualization: Gradient Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d65827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient field for f(x,y) = x^2 + y^2\n",
    "x = torch.linspace(-3, 3, 20)\n",
    "y = torch.linspace(-3, 3, 20)\n",
    "X, Y = torch.meshgrid(x, y, indexing='ij')\n",
    "\n",
    "# Compute gradient at each point\n",
    "U = 2 * X  # ∂f/∂x = 2x\n",
    "V = 2 * Y  # ∂f/∂y = 2y\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.quiver(X.numpy(), Y.numpy(), U.numpy(), V.numpy(), alpha=0.6)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Gradient Field of f(x,y) = x² + y²', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a71dc4",
   "metadata": {},
   "source": [
    "## 13. 3D Surface and Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7866bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D visualization of function and gradient\n",
    "x = torch.linspace(-2, 2, 50)\n",
    "y = torch.linspace(-2, 2, 50)\n",
    "X, Y = torch.meshgrid(x, y, indexing='ij')\n",
    "Z = X**2 + Y**2\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Surface plot\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('f(x,y)')\n",
    "ax1.set_title('f(x,y) = x² + y²')\n",
    "\n",
    "# Contour with gradient vectors\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(X.numpy(), Y.numpy(), Z.numpy(), levels=15, cmap='viridis')\n",
    "plt.colorbar(contour, ax=ax2)\n",
    "\n",
    "# Add some gradient vectors\n",
    "x_grad = torch.linspace(-2, 2, 10)\n",
    "y_grad = torch.linspace(-2, 2, 10)\n",
    "X_grad, Y_grad = torch.meshgrid(x_grad, y_grad, indexing='ij')\n",
    "U_grad = 2 * X_grad\n",
    "V_grad = 2 * Y_grad\n",
    "\n",
    "ax2.quiver(X_grad.numpy(), Y_grad.numpy(), U_grad.numpy(), V_grad.numpy(), alpha=0.6, color='red')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('Contours and Gradients')\n",
    "ax2.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f887963",
   "metadata": {},
   "source": [
    "## 14. Practical Application: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd58a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "torch.manual_seed(42)\n",
    "X = torch.randn(100, 1)\n",
    "y_true = 3 * X + 2 + 0.5 * torch.randn(100, 1)\n",
    "\n",
    "# Initialize parameters\n",
    "w = torch.zeros(1, 1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    y_pred = X @ w + b\n",
    "    loss = ((y_pred - y_true)**2).mean()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}, w = {w.item():.4f}, b = {b.item():.4f}\")\n",
    "\n",
    "print(f\"\\nFinal: w = {w.item():.4f}, b = {b.item():.4f}\")\n",
    "print(f\"Expected: w ≈ 3.0, b ≈ 2.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97cd515",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688196eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot data and fitted line\n",
    "axes[0].scatter(X.numpy(), y_true.numpy(), alpha=0.5, label='Data')\n",
    "X_line = torch.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "y_line = (X_line @ w + b).detach()\n",
    "axes[0].plot(X_line.numpy(), y_line.numpy(), 'r-', linewidth=2, label='Fitted Line')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Linear Regression')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot loss curve\n",
    "axes[1].plot(losses, linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss (MSE)')\n",
    "axes[1].set_title('Training Loss')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
